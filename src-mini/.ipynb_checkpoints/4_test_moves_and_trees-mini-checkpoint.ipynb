{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dynet\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0016647412116732802\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    (vocab, net_properties) = pickle.load(open('../data/mini/vocab_net.data', 'rb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.load('../data/mini/net.model')\n",
    "\n",
    "    writer = open('../data/mini/predictions.data', 'w')\n",
    "    feature_list = pickle.load(open( \"../data/mini/features/test.data\", \"rb\" ))\n",
    "\n",
    "    correct = 0\n",
    "    shiftstars, tps, fps, fns, shifts, tpss, fpss, fnss = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    for feature_set in feature_list:\n",
    "        pred = network.decode(feature_set[:-1])\n",
    "        writer.write(\"ground truth: \" + feature_set[-1] + \", prediction: \" + pred + \"\\n\\n\")\n",
    "\n",
    "        # count shift stars\n",
    "        if pred == \"shift star\":\n",
    "            if feature_set[-1] == \"shift star\":\n",
    "                shiftstars += 1\n",
    "                tpss += 1\n",
    "            else:\n",
    "                fpss += 1\n",
    "        if feature_set[-1] == \"shift star\" and pred != \"shift star\":\n",
    "            fnss += 1\n",
    "            shiftstars += 1\n",
    "\n",
    "        # count shifts\n",
    "        if pred == \"shift\":\n",
    "            if feature_set[-1] == \"shift\":\n",
    "                shifts += 1\n",
    "                tps += 1\n",
    "            else:\n",
    "                fps += 1\n",
    "        if feature_set[-1] == \"shift\" and pred != \"shift\":\n",
    "            fns += 1\n",
    "            shifts += 1\n",
    "\n",
    "        # total accuracy\n",
    "        if pred == feature_set[-1]:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "    print(\"accuracy: \" + str(float(correct)/len(feature_list)))\n",
    "\n",
    "#     print(\"----------------------------------------\")\n",
    "\n",
    "#     precision_ss = tpss / (tpss + fpss)\n",
    "#     recall_ss = tpss / (tpss + fnss)\n",
    "#     print(\"star precision: \" + str(precision_ss))\n",
    "#     print(\"star recall: \" + str(recall_ss))\n",
    "#     print(\"total stars: \" + str(shiftstars))\n",
    "#     print(\"star F1: \" + str( 2*(precision_ss * recall_ss) / (precision_ss + recall_ss) ))\n",
    "\n",
    "#     print(\"----------------------------------------\")\n",
    "\n",
    "#     precision_s = tps / (tps + fps)\n",
    "#     recall_s = tps / (tps + fns)\n",
    "#     print(\"shift precision: \" + str(precision_s))\n",
    "#     print(\"shift recall: \" + str(recall_s))\n",
    "#     print(\"total shifts: \" + str(shifts))\n",
    "#     print(\"shift F1: \" + str( 2*(precision_s * recall_s) / (precision_s + recall_s) ))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_star(s):\n",
    "    s = s.split()\n",
    "    s = list(filter(lambda x: '*' not in x, s))\n",
    "    return ' '.join(s)\n",
    "\n",
    "def action(b, s, p):\n",
    "    error = None\n",
    "\n",
    "    if p.split()[0] == 'shift':\n",
    "        if len(p.split()) > 1 and p.split()[1] == 'star':\n",
    "            s.append(Node('*'))\n",
    "            \n",
    "        # normal shift\n",
    "        try: s.append(b.pop(0))\n",
    "        except: error = 'pop on empty buffer'\n",
    "\n",
    "    elif p.split()[0] == 'unary':\n",
    "        n = Node(clean(p.split()[1]))\n",
    "        try:\n",
    "            n.l = s.pop()\n",
    "            s.append(n)\n",
    "        except: error = 'unary on empty stack'\n",
    "\n",
    "    else: # p.split()[0] == 'binary':\n",
    "        n = Node(clean(p.split()[1]))\n",
    "        try:\n",
    "            n.r, n.l = s.pop(), s.pop()\n",
    "            s.append(n)\n",
    "        except: error = 'binary on insufficient stack'\n",
    "\n",
    "    return b, s, error\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load the network\n",
    "    (vocab, net_properties) = pickle.load(open('../data/mini/vocab_net.data', 'rb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.load('../data/mini/net.model')\n",
    "\n",
    "    # open treebank for testing\n",
    "    treepath = \"../treebank/treebank_3/parsed/mrg/wsj/00\"\n",
    "\n",
    "    # open file and save as one large string\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(treepath):\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "        with open(treepath + \"/\" + filename, 'r') as f:\n",
    "            text += f.read().replace('\\n', '')\n",
    "\n",
    "    tree_string_list = []\n",
    "    s = []\n",
    "    start = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"(\":\n",
    "            s.append(\"(\")\n",
    "        elif text[i] == \")\":\n",
    "            s.pop()\n",
    "            if not s:\n",
    "                tree_string_list.append(text[start : i + 1])\n",
    "                start = i + 1\n",
    "\n",
    "    # turn tree strings into tree_list\n",
    "    tree_list = []\n",
    "    for t in tree_string_list:\n",
    "        tree_list.append((parse_tree(t[1:-1])))\n",
    "\n",
    "    # use inorder traveral to generate sentences from trees\n",
    "    sentences = []\n",
    "    for t in tree_list:\n",
    "        sentences.append(remove_star(inorder_sentence(t).lstrip()))\n",
    "\n",
    "    # testing\n",
    "\n",
    "    with open('../data/mini/tree_pred.txt', 'w') as outfile, open('../data/mini/evalb.txt', 'w') as evalb:\n",
    "        for s, t in zip(sentences, tree_list):\n",
    "            s = [clean(x) for x in s.split()]\n",
    "            \n",
    "            #debug\n",
    "#             print(' '.join(s) + '\\n') # print sentence\n",
    "            outfile.write(' '.join(s) + '\\n\\n')\n",
    "\n",
    "            # construct tree\n",
    "            buff = list(map(Node, s))\n",
    "            stack = []\n",
    "            infinite_loop_count = 0 # terminate after 100 moves\n",
    "            printed_from_error = False\n",
    "            while buff or len(stack) > 1: # end when buff consumed & stack has tree\n",
    "                \n",
    "                \n",
    "                # cast to string and predict\n",
    "                stack, buff = list(map(tree_to_str, stack)), list(map(tree_to_str, buff))\n",
    "                try: f = extract_features(datum(stack, buff, None))\n",
    "                except: \n",
    "#                     print('feature extraction error')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "                    \n",
    "\n",
    "                pred = network.decode(rearrange([0] + f)[:-1])\n",
    "                # outfile.write(str(f) + ' ' +  pred + '\\n')\n",
    "\n",
    "                # cast back to Node and complete action\n",
    "                stack, buff = list(map(Node, stack)), list(map(Node, buff))\n",
    "                buff, stack, error = action(buff, stack, pred)\n",
    "                if error:\n",
    "                    # outfile.write(error + '\\n')\n",
    "#                     print('Error: ' + error)\n",
    "#                     print(stack_to_str(stack) + '\\n')\n",
    "                    outfile.write('Error: ' + error + '\\n')\n",
    "                    outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    evalb.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "#                 print(pred + '\\n' + stack_to_str(stack) + '\\n')\n",
    "                outfile.write(pred + '\\n' + stack_to_str(stack) + '\\n\\n')\n",
    "                \n",
    "                infinite_loop_count += 1\n",
    "                if infinite_loop_count >= 150: \n",
    "#                     print('infinite loop error')\n",
    "#                     print(stack_to_str(stack) + '\\n')\n",
    "                    outfile.write('infinite loop error' + '\\n')\n",
    "                    outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    evalb.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "                \n",
    "#             if not printed_from_error: print(stack_to_str(stack) + '\\n')\n",
    "            if not printed_from_error: \n",
    "                outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "                evalb.write(stack_to_str(stack) + '\\n\\n')\n",
    "#             print('GROUND TRUTH:\\n' + tree_to_str(t) + '\\n')\n",
    "#             print('-------------------end of sentence-----------------\\n')\n",
    "            outfile.write('GROUND TRUTH:\\n' + tree_to_str(t) + '\\n\\n')\n",
    "            outfile.write('-------------------end of sentence-----------------\\n\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
