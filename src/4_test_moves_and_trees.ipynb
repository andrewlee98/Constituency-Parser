{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dynet\n",
    "import os\n",
    "import numpy as np\n",
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8189\n",
      "----------------------------------------\n",
      "star precision: 0.7443609022556391\n",
      "star recall: 0.6111111111111112\n",
      "total stars: 162\n",
      "star F1: 0.6711864406779662\n",
      "----------------------------------------\n",
      "shift precision: 0.851279000594884\n",
      "shift recall: 0.9340731070496083\n",
      "total shifts: 3064\n",
      "shift F1: 0.8907563025210083\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    (vocab, net_properties) = pickle.load(open('../data/vocab_net.data', 'rb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.load('../data/net.model')\n",
    "\n",
    "    writer = open('../data/predictions.data', 'w')\n",
    "    feature_list = pickle.load(open( \"../data/test.data\", \"rb\" ))\n",
    "\n",
    "    correct = 0\n",
    "    shiftstars, tps, fps, fns, shifts, tpss, fpss, fnss = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    for feature_set in feature_list:\n",
    "        pred = network.decode(feature_set[:-1])\n",
    "        writer.write(\"ground truth: \" + feature_set[-1] + \", prediction: \" + pred + \"\\n\\n\")\n",
    "\n",
    "        # count shift stars\n",
    "        if pred == \"shift star\":\n",
    "            if feature_set[-1] == \"shift star\":\n",
    "                shiftstars += 1\n",
    "                tpss += 1\n",
    "            else:\n",
    "                fpss += 1\n",
    "        if feature_set[-1] == \"shift star\" and pred != \"shift star\":\n",
    "            fnss += 1\n",
    "            shiftstars += 1\n",
    "\n",
    "        # count shifts\n",
    "        if pred == \"shift\":\n",
    "            if feature_set[-1] == \"shift\":\n",
    "                shifts += 1\n",
    "                tps += 1\n",
    "            else:\n",
    "                fps += 1\n",
    "        if feature_set[-1] == \"shift\" and pred != \"shift\":\n",
    "            fns += 1\n",
    "            shifts += 1\n",
    "\n",
    "        # total accuracy\n",
    "        if pred == feature_set[-1]:\n",
    "            correct += 1\n",
    "\n",
    "\n",
    "    print(\"accuracy: \" + str(float(correct)/len(feature_list)))\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    precision_ss = tpss / (tpss + fpss)\n",
    "    recall_ss = tpss / (tpss + fnss)\n",
    "    print(\"star precision: \" + str(precision_ss))\n",
    "    print(\"star recall: \" + str(recall_ss))\n",
    "    print(\"total stars: \" + str(shiftstars))\n",
    "    print(\"star F1: \" + str( 2*(precision_ss * recall_ss) / (precision_ss + recall_ss) ))\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    precision_s = tps / (tps + fps)\n",
    "    recall_s = tps / (tps + fns)\n",
    "    print(\"shift precision: \" + str(precision_s))\n",
    "    print(\"shift recall: \" + str(recall_s))\n",
    "    print(\"total shifts: \" + str(shifts))\n",
    "    print(\"shift F1: \" + str( 2*(precision_s * recall_s) / (precision_s + recall_s) ))\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_star(s):\n",
    "    s = s.split()\n",
    "    s = list(filter(lambda x: '*' not in x, s))\n",
    "    return ' '.join(s)\n",
    "\n",
    "def action(b, s, p):\n",
    "    error = None\n",
    "\n",
    "    if p.split()[0] == 'shift':\n",
    "        if len(p.split()) > 1 and p.split()[1] == 'star':\n",
    "            s.append(Node('*'))\n",
    "            \n",
    "        # normal shift\n",
    "        try: s.append(b.pop(0))\n",
    "        except: error = 'pop on empty buffer'\n",
    "\n",
    "    elif p.split()[0] == 'unary':\n",
    "        n = Node(clean(p.split()[1]))\n",
    "        try:\n",
    "            n.l = s.pop()\n",
    "            s.append(n)\n",
    "        except: error = 'unary on empty stack'\n",
    "\n",
    "    else: # p.split()[0] == 'binary':\n",
    "        n = Node(clean(p.split()[1]))\n",
    "        try:\n",
    "            n.r, n.l = s.pop(), s.pop()\n",
    "            s.append(n)\n",
    "        except: error = 'binary on insufficient stack'\n",
    "\n",
    "    return b, s, error\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # load the network\n",
    "    (vocab, net_properties) = pickle.load(open('../data/vocab_net.data', 'rb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.load('../data/net.model')\n",
    "\n",
    "    # open treebank for testing\n",
    "    treepath = \"../treebank/treebank_3/parsed/mrg/wsj/23\"\n",
    "\n",
    "    # open file and save as one large string\n",
    "    text = \"\"\n",
    "    for filename in os.listdir(treepath):\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "        with open(treepath + \"/\" + filename, 'r') as f:\n",
    "            text += f.read().replace('\\n', '')\n",
    "\n",
    "    tree_string_list = []\n",
    "    s = []\n",
    "    start = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"(\":\n",
    "            s.append(\"(\")\n",
    "        elif text[i] == \")\":\n",
    "            s.pop()\n",
    "            if not s:\n",
    "                tree_string_list.append(text[start : i + 1])\n",
    "                start = i + 1\n",
    "\n",
    "    # turn tree strings into tree_list\n",
    "    tree_list = []\n",
    "    for t in tree_string_list:\n",
    "        tree_list.append((parse_tree(t[1:-1])))\n",
    "\n",
    "    # use inorder traveral to generate sentences from trees\n",
    "    sentences = []\n",
    "    for t in tree_list:\n",
    "        sentences.append(remove_star(inorder_sentence(t).lstrip()))\n",
    "\n",
    "    # testing\n",
    "\n",
    "    with open('../data/tree_pred.txt', 'w') as outfile:\n",
    "        for s, t in zip(sentences, tree_list):\n",
    "            s = [clean(x) for x in s.split()]\n",
    "            \n",
    "            #debug\n",
    "#             print(' '.join(s) + '\\n') # print sentence\n",
    "            outfile.write(' '.join(s) + '\\n\\n')\n",
    "\n",
    "            # construct tree\n",
    "            buff = list(map(Node, s))\n",
    "            stack = []\n",
    "            infinite_loop_count = 0 # terminate after 100 moves\n",
    "            printed_from_error = False\n",
    "            while buff or len(stack) > 1: # end when buff consumed & stack has tree\n",
    "                \n",
    "                \n",
    "                # cast to string and predict\n",
    "                stack, buff = list(map(tree_to_str, stack)), list(map(tree_to_str, buff))\n",
    "                try: f = extract_features(datum(stack, buff, None))\n",
    "                except: \n",
    "#                     print('feature extraction error')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "                    \n",
    "\n",
    "                pred = network.decode(rearrange([0] + f)[:-1])\n",
    "                # outfile.write(str(f) + ' ' +  pred + '\\n')\n",
    "\n",
    "                # cast back to Node and complete action\n",
    "                stack, buff = list(map(Node, stack)), list(map(Node, buff))\n",
    "                buff, stack, error = action(buff, stack, pred)\n",
    "                if error:\n",
    "                    # outfile.write(error + '\\n')\n",
    "#                     print('Error: ' + error)\n",
    "#                     print(stack_to_str(stack) + '\\n')\n",
    "                    outfile.write('Error: ' + error + '\\n')\n",
    "                    outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "#                 print(pred + '\\n' + stack_to_str(stack) + '\\n')\n",
    "                outfile.write(pred + '\\n' + stack_to_str(stack) + '\\n\\n')\n",
    "                \n",
    "                infinite_loop_count += 1\n",
    "                if infinite_loop_count >= 150: \n",
    "#                     print('infinite loop error')\n",
    "#                     print(stack_to_str(stack) + '\\n')\n",
    "                    outfile.write('infinite loop error' + '\\n')\n",
    "                    outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "                    printed_from_error = True\n",
    "                    break\n",
    "                \n",
    "#             if not printed_from_error: print(stack_to_str(stack) + '\\n')\n",
    "            if not printed_from_error: outfile.write(stack_to_str(stack) + '\\n\\n')\n",
    "#             print('GROUND TRUTH:\\n' + tree_to_str(t) + '\\n')\n",
    "#             print('-------------------end of sentence-----------------\\n')\n",
    "            outfile.write('GROUND TRUTH:\\n' + tree_to_str(t) + '\\n\\n')\n",
    "            outfile.write('-------------------end of sentence-----------------\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
