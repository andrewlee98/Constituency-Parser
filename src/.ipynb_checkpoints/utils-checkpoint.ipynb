{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.l = None\n",
    "        self.r = None\n",
    "\n",
    "class datum:\n",
    "    def __init__(self, stack, buff, label):\n",
    "        self.stack = stack\n",
    "        self.buff = buff\n",
    "        self.label = label\n",
    "\n",
    "# generate sentences from the tree\n",
    "def inorder_sentence(root, s = \"\"):\n",
    "    if not root.l and not root.r:\n",
    "        s += \" \" + root.label\n",
    "        return s\n",
    "    if root.l:\n",
    "        s = inorder_sentence(root.l, s)\n",
    "    if root.r:\n",
    "        s = inorder_sentence(root.r, s)\n",
    "    return s\n",
    "\n",
    "# util to debug\n",
    "def tree_to_str(root, s = \"\"):\n",
    "    sr = \"\"\n",
    "    sl = \"\"\n",
    "    if root.l:\n",
    "        sl = tree_to_str(root.l, s)\n",
    "    if root.r:\n",
    "        sr = tree_to_str(root.r, s)\n",
    "    if root and not root.r and not root.l:\n",
    "        s += \" \" + clean(root.label)\n",
    "    elif root.label:\n",
    "        s += \" (\" + clean(root.label) + sl + sr + \")\"\n",
    "    return s\n",
    "\n",
    "def stack_to_str(s):\n",
    "    if not s: return '[]'\n",
    "    ret = \"[\"\n",
    "    for t in s[:-1]:\n",
    "        ret += tree_to_str(t) + \", \"\n",
    "    ret += tree_to_str(s[-1]) + \"]\"\n",
    "    return ret\n",
    "\n",
    "# level order print for debugging\n",
    "def level_order(root):\n",
    "    current_level = [root]\n",
    "    while current_level:\n",
    "        print(' '.join(str(node.label) for node in current_level))\n",
    "        next_level = list()\n",
    "        for n in current_level:\n",
    "            if n.l:\n",
    "                next_level.append(n.l)\n",
    "            if n.r:\n",
    "                next_level.append(n.r)\n",
    "            current_level = next_level\n",
    "\n",
    "def clean(s): # remove excess space\n",
    "        s = s.rstrip().lstrip()\n",
    "        return s\n",
    "\n",
    "# method for transforming one string into a tree\n",
    "def parse_tree(tree_str):\n",
    "    tree_str = clean(tree_str)\n",
    "    if tree_str[0] == \"(\": # remove surrounding parentheses\n",
    "        tree_str = tree_str[1:-1]\n",
    "    root = Node(tree_str.split()[0]) # set first word as root\n",
    "    tree_str = tree_str.split()[1:] # remove first (root) word\n",
    "    tree_str = ' '.join(tree_str) # convert back to string\n",
    "\n",
    "\n",
    "    stack = [] # use to keep track of parentheses\n",
    "    nested = False # boolean for if in nested statement\n",
    "    children = [] # list of strings for children\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tree_str): # collect children into list\n",
    "        # nested parentheses case\n",
    "        if tree_str[i] == \"(\":\n",
    "            stack.append(\"(\")\n",
    "            if not nested: # save index of outermost left paren\n",
    "                start_idx = i\n",
    "            nested = True\n",
    "        elif tree_str[i] == \")\" and nested:\n",
    "            stack.pop()\n",
    "            if not stack:\n",
    "                children.append(tree_str[start_idx:i + 1])\n",
    "                nested = False\n",
    "\n",
    "        # handle base case string\n",
    "        elif tree_str[i] not in \"() \\n\\t\" and not nested:\n",
    "            start_idx = i\n",
    "            while tree_str[i] not in \"() \\n\\t\" and i < len(tree_str) - 1:\n",
    "                i += 1\n",
    "            children.append(tree_str[start_idx:i + 1])\n",
    "        i += 1\n",
    "    # now the children list is complete, recurse\n",
    "    children = list(map(clean, children)) # clean whitespace from children\n",
    "    if len(children) == 1:\n",
    "        root.l = parse_tree(children[0])\n",
    "    elif len(children) == 2:\n",
    "        root.l = parse_tree(children[0])\n",
    "        root.r = parse_tree(children[1])\n",
    "    elif len(children) > 2: # binarize case\n",
    "        root.l = parse_tree(children[0])\n",
    "        binarize_str = \"(\" + root.label\n",
    "        binarize_str += \"_inner \" if \"_inner\" not in binarize_str else \" \"\n",
    "        binarize_str += ' '.join(children[1:]) + \")\"\n",
    "        root.r = parse_tree(binarize_str) # recursively binarize\n",
    "    return root\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, data_path):\n",
    "        feature_list = pickle.load(open( \"../data/all_features.data\", \"rb\" ))\n",
    "        word_count, actions, labels = defaultdict(int), set(), set()\n",
    "\n",
    "        for feats in feature_list:\n",
    "            actions.add(feats[-1])\n",
    "            for word in feats[12:-1]:\n",
    "                word_count[word] += 1\n",
    "            for t in feats[:12]:\n",
    "                labels.add(t)\n",
    "        actions = list(actions)\n",
    "        labels = list(labels)\n",
    "        words = [word for word in word_count.keys() if word_count[word] > 1]\n",
    "\n",
    "        self.words = ['<UNK>'] + words\n",
    "        self.word_dict = {word: i for i, word in enumerate(self.words)}\n",
    "\n",
    "        self.output_acts = list(actions)\n",
    "        self.output_act_dict = {a: i for i, a in enumerate(self.output_acts)}\n",
    "\n",
    "        self.feat_acts = list(labels)\n",
    "        self.feat_acts_dict = {a: i for i, a in enumerate(self.feat_acts)}\n",
    "\n",
    "    def tagid2tag_str(self, id):\n",
    "        return self.output_acts[id]\n",
    "\n",
    "    def tag2id(self, tag):\n",
    "        return self.output_act_dict[tag]\n",
    "\n",
    "    def feat_tag2id(self, tag):\n",
    "        return self.feat_acts_dict[tag]\n",
    "\n",
    "    def word2id(self, word):\n",
    "        return self.word_dict[word] if word in self.word_dict else self.word_dict['<UNK>']\n",
    "\n",
    "    def num_words(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def num_tag_feats(self):\n",
    "        return len(self.feat_acts)\n",
    "\n",
    "    def num_tags(self):\n",
    "        return len(self.output_acts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, vocab, properties):\n",
    "        self.properties = properties\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # first initialize a computation graph container (or model)\n",
    "        self.model = dynet.Model()\n",
    "\n",
    "        # assign the algorithm for backpropagation updates\n",
    "        self.updater = dynet.AdamTrainer(self.model)\n",
    "\n",
    "        # create embeddings for words and tag features.\n",
    "        self.word_embedding = self.model.add_lookup_parameters((vocab.num_words(), properties.word_embed_dim))\n",
    "        self.tag_embedding = self.model.add_lookup_parameters((vocab.num_tag_feats(), properties.pos_embed_dim))\n",
    "\n",
    "        # assign transfer function\n",
    "        self.transfer = dynet.rectify  # can be dynet.logistic or dynet.tanh as well\n",
    "\n",
    "        # define the input dimension for the embedding layer\n",
    "        # here we assume to see two words after and before and current word (meaning 5 word embeddings)\n",
    "        # and to see the last two predicted tags (meaning two tag embeddings)\n",
    "        self.input_dim = 15 * properties.word_embed_dim + 12 * properties.pos_embed_dim\n",
    "\n",
    "        # define the hidden layer\n",
    "        self.hidden_layer = self.model.add_parameters((properties.hidden_dim, self.input_dim))\n",
    "\n",
    "        # define the hidden layer bias term and initialize it as constant 0.2\n",
    "        self.hidden_layer_bias = self.model.add_parameters(properties.hidden_dim, init=dynet.ConstInitializer(0.2))\n",
    "\n",
    "        # define the output weight\n",
    "        self.output_layer = self.model.add_parameters((vocab.num_tags(), properties.hidden_dim))\n",
    "\n",
    "        # define the bias vector and initialize it as zero\n",
    "        self.output_bias = self.model.add_parameters(vocab.num_tags(), init=dynet.ConstInitializer(0))\n",
    "\n",
    "    def build_graph(self, features):\n",
    "        # extract word and tags ids\n",
    "        word_ids = [self.vocab.word2id(word_feat) for word_feat in features[12:-1]]\n",
    "        tag_ids = [self.vocab.feat_tag2id(tag_feat) for tag_feat in features[0:12]]\n",
    "\n",
    "        # extract word embeddings and tag embeddings from features\n",
    "        word_embeds = [self.word_embedding[wid] for wid in word_ids]\n",
    "        tag_embeds = [self.tag_embedding[tid] for tid in tag_ids]\n",
    "\n",
    "        # concatenating all features (recall that '+' for lists is equivalent to appending two lists)\n",
    "        embedding_layer = dynet.concatenate(word_embeds + tag_embeds)\n",
    "\n",
    "        # calculating the hidden layer\n",
    "        # .expr() converts a parameter to a matrix expression in dynet (its a dynet-specific syntax)\n",
    "        hidden = self.transfer(self.hidden_layer.expr() * embedding_layer + self.hidden_layer_bias.expr())\n",
    "\n",
    "        # calculating the output layer\n",
    "        output = self.output_layer.expr() * hidden + self.output_bias.expr()\n",
    "\n",
    "        # return the output as a dynet vector (expression)\n",
    "        return output\n",
    "\n",
    "    def train(self, train_file, epochs):\n",
    "        plot_on = False\n",
    "        # matplotlib config\n",
    "        loss_values = []\n",
    "        if plot_on:\n",
    "            plt.ion()\n",
    "            ax = plt.gca()\n",
    "            ax.set_xlim([0, 10])\n",
    "            ax.set_ylim([0, 5])\n",
    "            plt.title(\"Loss over time\")\n",
    "            plt.xlabel(\"Minibatch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print('started epoch', (i+1))\n",
    "            losses = []\n",
    "            train_data = pickle.load(open( \"../data/train.data\", \"rb\" ))\n",
    "\n",
    "            # shuffle the training data.\n",
    "            random.shuffle(train_data)\n",
    "\n",
    "            step = 0\n",
    "            for fl in train_data:\n",
    "                features, label = fl[:-1], fl[-1]\n",
    "                gold_label = self.vocab.tag2id(label)\n",
    "                result = self.build_graph(features)\n",
    "\n",
    "                # getting loss with respect to negative log softmax function and the gold label\n",
    "                loss = dynet.pickneglogsoftmax(result, gold_label)\n",
    "\n",
    "                # appending to the minibatch losses\n",
    "                losses.append(loss)\n",
    "                step += 1\n",
    "\n",
    "                if len(losses) >= self.properties.minibatch_size:\n",
    "                    # now we have enough loss values to get loss for minibatch\n",
    "                    minibatch_loss = dynet.esum(losses) / len(losses)\n",
    "\n",
    "                    # calling dynet to run forward computation for all minibatch items\n",
    "                    minibatch_loss.forward()\n",
    "\n",
    "                    # getting float value of the loss for current minibatch\n",
    "                    minibatch_loss_value = minibatch_loss.value()\n",
    "\n",
    "                    # printing info and plotting\n",
    "                    loss_values.append(minibatch_loss_value)\n",
    "                    if len(loss_values)%10==0:\n",
    "                        if plot_on:\n",
    "                            ax.set_xlim([0, len(loss_values)+10])\n",
    "                            ax.plot(loss_values)\n",
    "                            plt.draw()\n",
    "                            plt.pause(0.0001)\n",
    "                        progress = round(100 * float(step) / len(train_data), 2)\n",
    "                        print('current minibatch loss', minibatch_loss_value, 'progress:', progress, '%')\n",
    "\n",
    "                    # calling dynet to run backpropagation\n",
    "                    minibatch_loss.backward()\n",
    "\n",
    "                    # calling dynet to change parameter values with respect to current backpropagation\n",
    "                    self.updater.update()\n",
    "\n",
    "                    # empty the loss vector\n",
    "                    losses = []\n",
    "\n",
    "                    # refresh the memory of dynet\n",
    "                    dynet.renew_cg()\n",
    "\n",
    "            # there are still some minibatch items in the memory but they are smaller than the minibatch size\n",
    "            # so we ask dynet to forget them\n",
    "            dynet.renew_cg()\n",
    "\n",
    "    def decode(self, features):\n",
    "\n",
    "        # running forward\n",
    "        output = self.build_graph(features)\n",
    "\n",
    "        # getting list value of the output\n",
    "        scores = output.npvalue()\n",
    "\n",
    "        # getting best tag\n",
    "        best_tag_id = np.argmax(scores)\n",
    "\n",
    "        # assigning the best tag\n",
    "        pred = self.vocab.tagid2tag_str(best_tag_id)\n",
    "\n",
    "        # refresh dynet memory (computation graph)\n",
    "        dynet.renew_cg()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model.populate(filename)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetProperties:\n",
    "    def __init__(self, word_embed_dim, pos_embed_dim, hidden_dim, minibatch_size):\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.pos_embed_dim = pos_embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.minibatch_size = minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange(f):\n",
    "    # 0(5): label of stack[0] (\"<word>\" if word)\n",
    "    # 1(7): leftmost POS\n",
    "    # 2(9): rightmost POS\n",
    "    # 3(11): label of stack[1] (\"<word>\" if word)\n",
    "    # 4(13): leftmost POS\n",
    "    # 5(15): rightmost POS\n",
    "    # 6(17): label of stack[2] (\"<word>\" if word)\n",
    "    # 7(19): leftmost POS\n",
    "    # 8(21): rightmost POS\n",
    "    # 9(23): label of stack[3] (\"<word>\" if word)\n",
    "    # 10(25): leftmost POS\n",
    "    # 11(27): rightmost POS\n",
    "    labels = set([5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27])\n",
    "    new_list = []\n",
    "    for i in range(1, len(f)):\n",
    "        if i in labels:\n",
    "            new_list.append(f[i])\n",
    "    for i in range(1, len(f)):\n",
    "        if i not in labels: # and i not in prefix:\n",
    "            new_list.append(f[i])\n",
    "    new_list.append(f[0]) # append label to end\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left(t):\n",
    "    if not t.l.r and not t.l.l: # if l child is unary\n",
    "        return [t.label, unindex(t.l.label)]\n",
    "    else:\n",
    "        return get_left(t.l)\n",
    "\n",
    "def get_right(t):\n",
    "    if not t.l.r and not t.l.l: # if l child is unary\n",
    "        return [t.label, unindex(t.l.label)]\n",
    "    else:\n",
    "        if t.r: # handle strange case of \"(NP (NNP Moscow) ))\"\n",
    "            return get_right(t.r)\n",
    "        else:\n",
    "            return get_right(t.l)\n",
    "\n",
    "def unindex(a):\n",
    "    return a.split(\"/\")[0].rstrip().lstrip() # assume no words contain \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(d):\n",
    "    features = []\n",
    "    stack = d.stack[::-1]\n",
    "    buff = d.buff\n",
    "\n",
    "    # top four buffer words\n",
    "    for i in range(0,4):\n",
    "        if len(buff) > i:\n",
    "            features.append(unindex(buff[i]))\n",
    "        else:\n",
    "            features.append(\"<null>\")\n",
    "\n",
    "        # stack items\n",
    "    for i in range(0,4):\n",
    "        if len(stack) > i:\n",
    "            tree = parse_tree(stack[i])\n",
    "            if tree.l or tree.r: # label\n",
    "                features.append(remove_trailing(tree.label))\n",
    "                features.append(\"<label>\")\n",
    "            else: # word\n",
    "                features.append(\"<word>\")\n",
    "                features.append(unindex(tree.label))\n",
    "\n",
    "\n",
    "            if tree.l and tree.r: # binary rule\n",
    "                # assume a depth of 3 at least\n",
    "                features.extend(get_left(tree.l))\n",
    "                features.extend(get_right(tree.r))\n",
    "            else:\n",
    "                features.extend([\"<null>\"] * 4)\n",
    "        else:\n",
    "            features.extend([\"<null>\"] * 6)\n",
    "\n",
    "    return features\n",
    "\n",
    "def remove_trailing(label):\n",
    "    if label[-1] == label[0] and label[0] == '-': return label[1:-1]\n",
    "    return ((label.split(\"-\")[0]).split('_')[0]).split('=')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
