{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dynet\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "%run utils.ipynb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    t0 = time.time()\n",
    "\n",
    "    we = 100\n",
    "    pe = 50\n",
    "    hidden = 200\n",
    "    minibatch = 1000\n",
    "    epochs = 5\n",
    "\n",
    "    net_properties = NetProperties(we, pe, hidden, minibatch)\n",
    "    vocab = Vocab(\"../data/train.data\")\n",
    "    \n",
    "    pickle.dump((vocab, net_properties), open(\"../data/vocab_net.data\", 'wb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.train(\"../data/train.data\", epochs)\n",
    "    network.save(\"../data/net.model\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"runtime: \" + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started epoch 1\n",
      "current minibatch loss 4.664115905761719 progress: 0.26 %\n",
      "current minibatch loss 4.606052398681641 progress: 0.52 %\n",
      "current minibatch loss 4.550240516662598 progress: 0.77 %\n",
      "current minibatch loss 4.4857025146484375 progress: 1.03 %\n",
      "current minibatch loss 4.432664394378662 progress: 1.29 %\n",
      "current minibatch loss 4.369981288909912 progress: 1.55 %\n",
      "current minibatch loss 4.3154215812683105 progress: 1.8 %\n",
      "current minibatch loss 4.251251220703125 progress: 2.06 %\n",
      "current minibatch loss 4.212245464324951 progress: 2.32 %\n",
      "current minibatch loss 4.172063827514648 progress: 2.58 %\n",
      "current minibatch loss 4.136328220367432 progress: 2.83 %\n",
      "current minibatch loss 4.090292930603027 progress: 3.09 %\n",
      "current minibatch loss 4.048352241516113 progress: 3.35 %\n",
      "current minibatch loss 4.021552562713623 progress: 3.61 %\n",
      "current minibatch loss 3.9644293785095215 progress: 3.86 %\n",
      "current minibatch loss 3.9202497005462646 progress: 4.12 %\n",
      "current minibatch loss 3.946505546569824 progress: 4.38 %\n",
      "current minibatch loss 3.8687024116516113 progress: 4.64 %\n",
      "current minibatch loss 3.829401969909668 progress: 4.89 %\n",
      "current minibatch loss 3.796079397201538 progress: 5.15 %\n",
      "current minibatch loss 3.7890443801879883 progress: 5.41 %\n",
      "current minibatch loss 3.7614452838897705 progress: 5.67 %\n",
      "current minibatch loss 3.698427200317383 progress: 5.92 %\n",
      "current minibatch loss 3.724069595336914 progress: 6.18 %\n",
      "current minibatch loss 3.6417064666748047 progress: 6.44 %\n",
      "current minibatch loss 3.6798059940338135 progress: 6.7 %\n",
      "current minibatch loss 3.6234889030456543 progress: 6.95 %\n",
      "current minibatch loss 3.6153111457824707 progress: 7.21 %\n",
      "current minibatch loss 3.606966018676758 progress: 7.47 %\n",
      "current minibatch loss 3.555218458175659 progress: 7.73 %\n",
      "current minibatch loss 3.537470579147339 progress: 7.98 %\n",
      "current minibatch loss 3.4861185550689697 progress: 8.24 %\n",
      "current minibatch loss 3.5538268089294434 progress: 8.5 %\n",
      "current minibatch loss 3.526407241821289 progress: 8.76 %\n",
      "current minibatch loss 3.4482672214508057 progress: 9.01 %\n",
      "current minibatch loss 3.4519340991973877 progress: 9.27 %\n",
      "current minibatch loss 3.377732276916504 progress: 9.53 %\n",
      "current minibatch loss 3.4373254776000977 progress: 9.79 %\n",
      "current minibatch loss 3.3185174465179443 progress: 10.04 %\n",
      "current minibatch loss 3.290696620941162 progress: 10.3 %\n",
      "current minibatch loss 3.3095874786376953 progress: 10.56 %\n",
      "current minibatch loss 3.312872886657715 progress: 10.82 %\n",
      "current minibatch loss 3.269888401031494 progress: 11.07 %\n",
      "current minibatch loss 3.300891160964966 progress: 11.33 %\n",
      "current minibatch loss 3.1999857425689697 progress: 11.59 %\n",
      "current minibatch loss 3.2594966888427734 progress: 11.85 %\n",
      "current minibatch loss 3.1978678703308105 progress: 12.1 %\n",
      "current minibatch loss 3.1923580169677734 progress: 12.36 %\n",
      "current minibatch loss 3.2203547954559326 progress: 12.62 %\n",
      "current minibatch loss 3.131964683532715 progress: 12.88 %\n",
      "current minibatch loss 3.172943353652954 progress: 13.13 %\n",
      "current minibatch loss 3.137009620666504 progress: 13.39 %\n",
      "current minibatch loss 3.1318185329437256 progress: 13.65 %\n",
      "current minibatch loss 3.060650587081909 progress: 13.91 %\n",
      "current minibatch loss 3.0152742862701416 progress: 14.16 %\n",
      "current minibatch loss 3.1017794609069824 progress: 14.42 %\n",
      "current minibatch loss 3.0719969272613525 progress: 14.68 %\n",
      "current minibatch loss 3.0814061164855957 progress: 14.94 %\n",
      "current minibatch loss 3.029329538345337 progress: 15.19 %\n",
      "current minibatch loss 2.9554593563079834 progress: 15.45 %\n",
      "current minibatch loss 3.0648856163024902 progress: 15.71 %\n",
      "current minibatch loss 3.0078327655792236 progress: 15.97 %\n",
      "current minibatch loss 2.932896852493286 progress: 16.22 %\n",
      "current minibatch loss 3.0596885681152344 progress: 16.48 %\n",
      "current minibatch loss 2.893357515335083 progress: 16.74 %\n",
      "current minibatch loss 2.9258170127868652 progress: 17.0 %\n",
      "current minibatch loss 2.974954605102539 progress: 17.25 %\n",
      "current minibatch loss 2.9104325771331787 progress: 17.51 %\n",
      "current minibatch loss 2.886380434036255 progress: 17.77 %\n",
      "current minibatch loss 2.965909719467163 progress: 18.03 %\n",
      "current minibatch loss 2.8041365146636963 progress: 18.28 %\n",
      "current minibatch loss 2.8317995071411133 progress: 18.54 %\n",
      "current minibatch loss 2.8065378665924072 progress: 18.8 %\n",
      "current minibatch loss 2.773862600326538 progress: 19.06 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6fd1a7e30b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-074d4a4a528b>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/vocab_net.data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/train.data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/net.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-609703fa08d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_file, epochs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mgold_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;31m# getting loss with respect to negative log softmax function and the gold label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-609703fa08d1>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# extract word embeddings and tag embeddings from features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mword_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtag_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# concatenating all features (recall that '+' for lists is equivalent to appending two lists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-609703fa08d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# extract word embeddings and tag embeddings from features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mword_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtag_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# concatenating all features (recall that '+' for lists is equivalent to appending two lists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocab(\"../data/train.data\")\n",
    "# print(\"words: \" + str(vocab.words) + \"\\n\\n\")\n",
    "# print(\"actions: \" + str(vocab.output_acts) + \"\\n\\n\")\n",
    "# print(\"labels: \" + str(vocab.feat_acts) + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
