{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0... 100... 200... 300... 400... 500... 600... 700... 800... 900... 1000... 1100... 1200... 1300... 1400... 1500... 1600... 1700... 1800... 1900... 2000... 2100... 2200... 2300... 2400... 2500... 2600... 2700... 2800... 2900... 3000... 3100... 3200... 3300... 3400... 3500... 3600... 3700... 3800... 3900... 4000... 4100... 4200... 4300... 4400... 4500... 4600... 4700... 4800... 4900... 5000... 5100... 5200... 5300... 5400... 5500... 5600... 5700... 5800... 5900... 6000... 6100... 6200... 6300... 6400... 6500... 6600... 6700... 6800... 6900... 7000... 7100... 7200... 7300... 7400... 7500... 7600... 7700... 7800... 7900... 8000... 8100... 8200... 8300... 8400... 8500... 8600... 8700... 8800... 8900... 9000... 9100... 9200... 9300... 9400... 9500... 9600... 9700... 9800... 9900... \n",
      "runtime: 262.895879983902\n"
     ]
    }
   ],
   "source": [
    "# usage: py gen.py\n",
    "# creates all.data file which contains sequence of shift reduce actions\n",
    "\n",
    "import os\n",
    "from utils import *\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# indexes words in sentence to prevent duplicates\n",
    "def idx_tree(root, i = 0, star = 0):\n",
    "    if not root.l and not root.r:\n",
    "        if \"*\" not in root.label:\n",
    "            root.label += \"/\" + str(i)\n",
    "            i += 1\n",
    "        else:\n",
    "            root.label += \"/\" + str(star)\n",
    "            star +=1\n",
    "        return (root, i, star)\n",
    "    if root.l:\n",
    "        (root.l, i, star) = idx_tree(root.l, i, star)\n",
    "    if root.r:\n",
    "        (root.r, i, star) = idx_tree(root.r, i, star)\n",
    "    return (root, i, star)\n",
    "\n",
    "# create stack/buffer actions from sentence and tree\n",
    "def generate_actions(t, s):\n",
    "\n",
    "    def match_tree(t1, t2): # subroutine for checking matching trees\n",
    "        if t1 is None and t2 is None: # base case\n",
    "            return True\n",
    "        if t1 is not None and t2 is not None:\n",
    "            return (t1.label == t2.label) and \\\n",
    "            match_tree(t1.l, t2.l) and \\\n",
    "            match_tree(t1.r, t2.r)\n",
    "        return False\n",
    "\n",
    "    def binary_label_dfs(root, s0, s1): # subroutine for determining the label\n",
    "\n",
    "        if match_tree(root.l, s0) and match_tree(root.r, s1): # base case\n",
    "            return root.label\n",
    "        if root.l:\n",
    "            left_label = binary_label_dfs(root.l, s0, s1)\n",
    "            if left_label:\n",
    "                return left_label\n",
    "        if root.r:\n",
    "            right_label = binary_label_dfs(root.r, s0, s1)\n",
    "            return right_label if right_label else None\n",
    "\n",
    "    def unary_label_dfs(root, s0): # subroutine for determining the label\n",
    "        if match_tree(root.l, s0) and root.r == None: # base case\n",
    "            return root.label\n",
    "\n",
    "        # recursive calls\n",
    "        child_label = None\n",
    "        if root.l:\n",
    "            child_label = unary_label_dfs(root.l, s0)\n",
    "\n",
    "        if root.r and not child_label:\n",
    "            child_label = unary_label_dfs(root.r, s0)\n",
    "\n",
    "        if child_label:\n",
    "            return child_label\n",
    "\n",
    "    ret = []\n",
    "    buff = list(map(Node, s.split()[::-1])) # reverse sentence for O(1) pop\n",
    "    stack = []\n",
    "\n",
    "    while buff or len(stack) > 1: # end when buffer consumed & stack has tree\n",
    "        # print(stack_to_str(stack))\n",
    "\n",
    "        # write the stack and buffer before action is performed\n",
    "        st = list(map(lambda x: tree_to_str(x), stack))\n",
    "        bu = list(map(lambda x: tree_to_str(x), buff[::-1]))\n",
    "        final_label = \"\"\n",
    "        final_action = \"\"\n",
    "        templab = 'NOTHING'\n",
    "        # try to reduce top two items\n",
    "        if len(stack) > 1: # reduce\n",
    "            left = stack[len(stack) - 2]\n",
    "            right = stack[len(stack) - 1]\n",
    "            new_node = Node(binary_label_dfs(t, left, right))\n",
    "            if new_node.label: # found a matching reduce\n",
    "                final_label = new_node.label\n",
    "\n",
    "                #TEMP\n",
    "                lab = final_label.replace('=', '-').split('-')\n",
    "                templab = lab\n",
    "                if lab[0]: final_label = lab[0]\n",
    "                else: final_label = lab[1]\n",
    "\n",
    "                final_action = \"binary\"\n",
    "                new_node.r = stack.pop()\n",
    "                new_node.l = stack.pop()\n",
    "                stack.append(new_node)\n",
    "            else: # try to unary reduce\n",
    "                child = stack[len(stack) - 1]\n",
    "                new_node = Node(unary_label_dfs(t, child))\n",
    "                if new_node.label: # found a unary reduce\n",
    "                    final_label = new_node.label\n",
    "\n",
    "                    #TEMP\n",
    "                    lab = final_label.replace('=', '-').split('-')\n",
    "                    templab = lab\n",
    "                    if lab[0]: final_label = lab[0]\n",
    "                    else: final_label = lab[1]\n",
    "\n",
    "                    final_action = \"unary\"\n",
    "                    new_node.l = stack.pop()\n",
    "                    stack.append(new_node)\n",
    "                else: # shift\n",
    "                    final_action = \"shift\"\n",
    "                    if \"*\" in buff[-1].label:\n",
    "                        final_action += \" star\"\n",
    "                    stack.append(buff.pop())\n",
    "        elif len(stack) == 1: # just try unary reduce\n",
    "            child = stack[len(stack) - 1]\n",
    "            new_node = Node(unary_label_dfs(t, child))\n",
    "            if new_node.label: # found a unary reduce\n",
    "                final_label = new_node.label\n",
    "\n",
    "                #TEMP\n",
    "                lab = final_label.replace('=', '-').split('-')\n",
    "                templab = lab\n",
    "                if lab[0]: final_label = lab[0] # for cases like -PRT-\n",
    "                else: final_label = lab[1]\n",
    "\n",
    "                final_action = \"unary\"\n",
    "                new_node.l = stack.pop()\n",
    "                stack.append(new_node)\n",
    "            else: # shift\n",
    "                final_action = \"shift\"\n",
    "                if \"*\" in buff[-1].label:\n",
    "                        final_action += \" star\"\n",
    "                stack.append(buff.pop())\n",
    "        else: # shift\n",
    "            final_action = \"shift\"\n",
    "            if \"*\" in buff[-1].label:\n",
    "                        final_action += \" star\"\n",
    "            stack.append(buff.pop())\n",
    "\n",
    "        # append all changes\n",
    "        act = final_action\n",
    "        lab = final_label\n",
    "        if lab == '' and templab != 'NOTHING': print(templab)\n",
    "\n",
    "        if act == 'shift' or act == 'shift star': ret.append(datum(st, bu, act))\n",
    "        else: ret.append(datum(st, bu, act + \" \" + lab))\n",
    "\n",
    "        # if act + \" \" + lab == 'unary ': print('what the fuck')\n",
    "    return ret\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t0 = time.time()\n",
    "    treepath = \"../treebank/treebank_3/parsed/mrg/wsj/\"\n",
    "    outpath = \"../data/\"\n",
    "\n",
    "    # open file and save as one large string\n",
    "    text = \"\"\n",
    "    for folder in os.listdir(treepath):\n",
    "        if folder.startswith('.'):\n",
    "            continue\n",
    "        for filename in os.listdir(treepath + folder):\n",
    "            if filename.startswith('.'):\n",
    "                continue\n",
    "            with open(treepath + folder + \"/\" + filename, 'r') as f:\n",
    "                text += f.read().replace('\\n', '')\n",
    "        #break # test only one folder for speed\n",
    "\n",
    "    tree_string_list = []\n",
    "    s = []\n",
    "    start = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \"(\":\n",
    "            s.append(\"(\")\n",
    "        elif text[i] == \")\":\n",
    "            s.pop()\n",
    "            if not s:\n",
    "                tree_string_list.append(text[start : i + 1])\n",
    "                start = i + 1\n",
    "\n",
    "\n",
    "    # turn tree strings into tree_list\n",
    "    tree_list = []\n",
    "    for t in tree_string_list:\n",
    "        tree_list.append((parse_tree(t[1:-1])))\n",
    "\n",
    "    tree_list = list(map(lambda x: idx_tree(x)[0], tree_list))\n",
    "\n",
    "    # use inorder traveral to generate sentences from trees\n",
    "    sentences = []\n",
    "    for t in tree_list:\n",
    "        sentences.append(inorder_sentence(t).lstrip()) # extra space on left\n",
    "\n",
    "    idx = 0\n",
    "    output_list = []\n",
    "    with open(outpath + 'actions.data', 'wb') as f:\n",
    "        for t, s in zip(tree_list[1:], sentences[1:]):\n",
    "            dat = generate_actions(t, s)\n",
    "            output_list.extend(dat)\n",
    "            if idx % 100 == 0: print(str(idx) + \"...\" , end = ' ', flush = True)\n",
    "            idx += 1\n",
    "            if idx == 10000: break # cut the test data short\n",
    "        print()\n",
    "        pickle.dump(output_list, f)\n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"runtime: \" + str(total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10000...20000...30000...40000...50000...60000...70000...80000...90000...100000...110000...120000...130000...140000...150000...160000...170000...180000...190000...200000...210000...220000...230000...240000...250000..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-d8b5b0d7aa59>\", line 118, in <module>\n",
      "    features  = [((d.label.split(\"-\")[0]).split('_')[0]).split('=')[0]] + extract_features(d)\n",
      "  File \"<ipython-input-2-d8b5b0d7aa59>\", line 75, in extract_features\n",
      "    features.append(unindex(buff[i]))\n",
      "  File \"<ipython-input-2-d8b5b0d7aa59>\", line 65, in unindex\n",
      "    return a.split(\"/\")[0].rstrip().lstrip() # assume no words contain \"/\"\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 1338, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 1298, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 583, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 626, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 595, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/usr/lib/python3.4/inspect.py\", line 580, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/awl2144/.virtualenvs/test_py3/lib/python3.4/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import pickle\n",
    "\n",
    "# feature list for one action:\n",
    "# 0: ground truth action\n",
    "#----------------------------------------\n",
    "# 1-4: top four words on buffer, <null> if they don't exist\n",
    "#----------------------------------------\n",
    "# 5: label of stack[0] (\"<word>\" if word)\n",
    "# 6: word of stack[0] (\"<label>\" if constituent label)\n",
    "# 7-10: leftmost POS and word, rightmost POS and word\n",
    "#----------------------------------------\n",
    "# 11: label of stack[1] (\"<word>\" if word)\n",
    "# 12: word of stack[1] (\"<label>\" if constituent label)\n",
    "# 13-16: leftmost POS and word, rightmost POS and word\n",
    "#----------------------------------------\n",
    "# 17: label of stack[2] (\"<word>\" if word)\n",
    "# 18: word of stack[2] (\"<label>\" if constituent label)\n",
    "# 19-22: leftmost POS and word, rightmost POS and word\n",
    "#----------------------------------------\n",
    "# 23: label of stack[3] (\"<word>\" if word)\n",
    "# 24: word of stack[3] (\"<label>\" if constituent label)\n",
    "# 25-28: leftmost POS and word, rightmost POS and word\n",
    "\n",
    "def rearrange(f):\n",
    "    # 0(5): label of stack[0] (\"<word>\" if word)\n",
    "    # 1(7): leftmost POS\n",
    "    # 2(9): rightmost POS\n",
    "    # 3(11): label of stack[1] (\"<word>\" if word)\n",
    "    # 4(13): leftmost POS\n",
    "    # 5(15): rightmost POS\n",
    "    # 6(17): label of stack[2] (\"<word>\" if word)\n",
    "    # 7(19): leftmost POS\n",
    "    # 8(21): rightmost POS\n",
    "    # 9(23): label of stack[3] (\"<word>\" if word)\n",
    "    # 10(25): leftmost POS\n",
    "    # 11(27): rightmost POS\n",
    "    labels = set([5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27])\n",
    "    new_list = []\n",
    "    for i in range(1, len(f)):\n",
    "        if i in labels:\n",
    "            new_list.append(f[i])\n",
    "    for i in range(1, len(f)):\n",
    "        if i not in labels: # and i not in prefix:\n",
    "            new_list.append(f[i])\n",
    "    new_list.append(f[0]) # append label to end\n",
    "    return new_list\n",
    "\n",
    "def get_left(t):\n",
    "    if not t.l.r and not t.l.l: # if l child is unary\n",
    "        return [t.label, unindex(t.l.label)]\n",
    "    else:\n",
    "        return get_left(t.l)\n",
    "\n",
    "def get_right(t):\n",
    "    if not t.l.r and not t.l.l: # if l child is unary\n",
    "        return [t.label, unindex(t.l.label)]\n",
    "    else:\n",
    "        if t.r: # handle strange case of \"(NP (NNP Moscow) ))\"\n",
    "            return get_right(t.r)\n",
    "        else:\n",
    "            return get_right(t.l)\n",
    "\n",
    "def unindex(a):\n",
    "    return a.split(\"/\")[0].rstrip().lstrip() # assume no words contain \"/\"\n",
    "\n",
    "def extract_features(d):\n",
    "    features = []\n",
    "    stack = d.stack[::-1]\n",
    "    buff = d.buff\n",
    "\n",
    "    # top four buffer words\n",
    "    for i in range(0,4):\n",
    "        if len(buff) > i:\n",
    "            features.append(unindex(buff[i]))\n",
    "        else:\n",
    "            features.append(\"<null>\")\n",
    "\n",
    "        # stack items\n",
    "    for i in range(0,4):\n",
    "        if len(stack) > i:\n",
    "            tree = parse_tree(stack[i])\n",
    "            if tree.l or tree.r: # label\n",
    "                features.append(tree.label)\n",
    "                features.append(\"<label>\")\n",
    "            else: # word\n",
    "                features.append(\"<word>\")\n",
    "                features.append(unindex(tree.label))\n",
    "\n",
    "\n",
    "            if tree.l and tree.r: # binary rule\n",
    "                # assume a depth of 3 at least\n",
    "                features.extend(get_left(tree.l))\n",
    "                features.extend(get_right(tree.r))\n",
    "            else:\n",
    "                features.extend([\"<null>\"] * 4)\n",
    "        else:\n",
    "            features.extend([\"<null>\"] * 6)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datapath = \"../data/actions.data\"\n",
    "    outpath = \"../data/\"\n",
    "    debug = open(\"feat_labels.log\", \"w\")\n",
    "\n",
    "    # open file and save as one large string\n",
    "    data_list = pickle.load(open(datapath, 'rb'))\n",
    "\n",
    "    final_list = [] # list of lists of features\n",
    "    final_list_read = [] # list of lists of features for debugging\n",
    "    i = 0\n",
    "    for d in data_list:\n",
    "        # if not d: continue# gets rid of weird empty string error\n",
    "\n",
    "        features  = [((d.label.split(\"-\")[0]).split('_')[0]).split('=')[0]] + extract_features(d)\n",
    "        final_list.append(rearrange(features))\n",
    "        final_list_read.append(features)\n",
    "        if i % 10000 == 0: print(str(i) + '...', end='', flush=True)\n",
    "        i += 1\n",
    "\n",
    "    print(\"Feature vectors: \" + str(len(final_list)))\n",
    "\n",
    "    with open(outpath + \"train.data\", \"wb\") as f:\n",
    "        pickle.dump(final_list[10000:], f)\n",
    "\n",
    "    with open(outpath + \"test.data\", \"wb\") as f:\n",
    "        pickle.dump(final_list[:10000], f)\n",
    "\n",
    "    with open(outpath + \"all_features.data\", \"wb\") as f:\n",
    "        pickle.dump(final_list, f)\n",
    "\n",
    "    # write in readable form\n",
    "    i = 1\n",
    "    with open(outpath + \"features_read.txt\", \"w\") as f:\n",
    "        for fl1, fl2 in zip(final_list, final_list_read):\n",
    "            f.write(str(fl1[:12]) + \"\\n\")\n",
    "            f.write(str(fl1[12:]) + \"\\n\\n\")\n",
    "\n",
    "            f.write(str(fl2[0:1]) + \"\\n\")\n",
    "            f.write(str(fl2[1:5]) + \"\\n\")\n",
    "            f.write(str(fl2[5:11]) + \"\\n\")\n",
    "            f.write(str(fl2[11:17]) + \"\\n\")\n",
    "            f.write(str(fl2[17:23]) + \"\\n\")\n",
    "            f.write(str(fl2[23:29]) + \"\\n\")\n",
    "            i += 1\n",
    "            if i == 5000: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetProperties:\n",
    "    def __init__(self, word_embed_dim, pos_embed_dim, hidden_dim, minibatch_size):\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.pos_embed_dim = pos_embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "import dynet as dynet\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, vocab, properties):\n",
    "        self.properties = properties\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # first initialize a computation graph container (or model).\n",
    "        self.model = dynet.Model()\n",
    "\n",
    "        # assign the algorithm for backpropagation updates.\n",
    "        self.updater = dynet.AdamTrainer(self.model)\n",
    "\n",
    "        # create embeddings for words and tag features.\n",
    "        self.word_embedding = self.model.add_lookup_parameters((vocab.num_words(), properties.word_embed_dim))\n",
    "        self.tag_embedding = self.model.add_lookup_parameters((vocab.num_tag_feats(), properties.pos_embed_dim))\n",
    "\n",
    "        # assign transfer function\n",
    "        self.transfer = dynet.rectify  # can be dynet.logistic or dynet.tanh as well.\n",
    "\n",
    "        # define the input dimension for the embedding layer.\n",
    "        # here we assume to see two words after and before and current word (meaning 5 word embeddings)\n",
    "        # and to see the last two predicted tags (meaning two tag embeddings)\n",
    "        self.input_dim = 15 * properties.word_embed_dim + 12 * properties.pos_embed_dim\n",
    "\n",
    "        # define the hidden layer.\n",
    "        self.hidden_layer = self.model.add_parameters((properties.hidden_dim, self.input_dim))\n",
    "\n",
    "        # define the hidden layer bias term and initialize it as constant 0.2.\n",
    "        self.hidden_layer_bias = self.model.add_parameters(properties.hidden_dim, init=dynet.ConstInitializer(0.2))\n",
    "\n",
    "        # define the output weight.\n",
    "        self.output_layer = self.model.add_parameters((vocab.num_tags(), properties.hidden_dim))\n",
    "\n",
    "        # define the bias vector and initialize it as zero.\n",
    "        self.output_bias = self.model.add_parameters(vocab.num_tags(), init=dynet.ConstInitializer(0))\n",
    "\n",
    "    def build_graph(self, features):\n",
    "        # extract word and tags ids\n",
    "        word_ids = [self.vocab.word2id(word_feat) for word_feat in features[12:-1]]\n",
    "        tag_ids = [self.vocab.feat_tag2id(tag_feat) for tag_feat in features[0:12]]\n",
    "\n",
    "        # extract word embeddings and tag embeddings from features\n",
    "        word_embeds = [self.word_embedding[wid] for wid in word_ids]\n",
    "        tag_embeds = [self.tag_embedding[tid] for tid in tag_ids]\n",
    "\n",
    "        # concatenating all features (recall that '+' for lists is equivalent to appending two lists)\n",
    "        embedding_layer = dynet.concatenate(word_embeds + tag_embeds)\n",
    "\n",
    "        # calculating the hidden layer\n",
    "        # .expr() converts a parameter to a matrix expression in dynet (its a dynet-specific syntax).\n",
    "        hidden = self.transfer(self.hidden_layer.expr() * embedding_layer + self.hidden_layer_bias.expr())\n",
    "\n",
    "        # calculating the output layer\n",
    "        output = self.output_layer.expr() * hidden + self.output_bias.expr()\n",
    "\n",
    "        # return the output as a dynet vector (expression)\n",
    "        return output\n",
    "\n",
    "    def train(self, train_file, epochs):\n",
    "        plot_on = False\n",
    "        # matplotlib config\n",
    "        loss_values = []\n",
    "        if plot_on:\n",
    "            plt.ion()\n",
    "            ax = plt.gca()\n",
    "            ax.set_xlim([0, 10])\n",
    "            ax.set_ylim([0, 5])\n",
    "            plt.title(\"Loss over time\")\n",
    "            plt.xlabel(\"Minibatch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "\n",
    "        for i in range(epochs):\n",
    "            print('started epoch', (i+1))\n",
    "            losses = []\n",
    "            train_data = pickle.load(open( \"../data/train.data\", \"rb\" ))\n",
    "\n",
    "            # shuffle the training data.\n",
    "            random.shuffle(train_data)\n",
    "\n",
    "            step = 0\n",
    "            for fl in train_data:\n",
    "                features, label = fl[:-1], fl[-1]\n",
    "                gold_label = self.vocab.tag2id(label)\n",
    "                result = self.build_graph(features)\n",
    "\n",
    "                # getting loss with respect to negative log softmax function and the gold label.\n",
    "                loss = dynet.pickneglogsoftmax(result, gold_label)\n",
    "\n",
    "                # appending to the minibatch losses\n",
    "                losses.append(loss)\n",
    "                step += 1\n",
    "\n",
    "                if len(losses) >= self.properties.minibatch_size:\n",
    "                    # now we have enough loss values to get loss for minibatch\n",
    "                    minibatch_loss = dynet.esum(losses) / len(losses)\n",
    "\n",
    "                    # calling dynet to run forward computation for all minibatch items\n",
    "                    minibatch_loss.forward()\n",
    "\n",
    "                    # getting float value of the loss for current minibatch\n",
    "                    minibatch_loss_value = minibatch_loss.value()\n",
    "\n",
    "                    # printing info and plotting\n",
    "                    loss_values.append(minibatch_loss_value)\n",
    "                    if len(loss_values)%10==0:\n",
    "                        if plot_on:\n",
    "                            ax.set_xlim([0, len(loss_values)+10])\n",
    "                            ax.plot(loss_values)\n",
    "                            plt.draw()\n",
    "                            plt.pause(0.0001)\n",
    "                        progress = round(100 * float(step) / len(train_data), 2)\n",
    "                        print('current minibatch loss', minibatch_loss_value, 'progress:', progress, '%')\n",
    "\n",
    "                    # calling dynet to run backpropagation\n",
    "                    minibatch_loss.backward()\n",
    "\n",
    "                    # calling dynet to change parameter values with respect to current backpropagation\n",
    "                    self.updater.update()\n",
    "\n",
    "                    # empty the loss vector\n",
    "                    losses = []\n",
    "\n",
    "                    # refresh the memory of dynet\n",
    "                    dynet.renew_cg()\n",
    "\n",
    "            # there are still some minibatch items in the memory but they are smaller than the minibatch size\n",
    "            # so we ask dynet to forget them\n",
    "            dynet.renew_cg()\n",
    "\n",
    "    def decode(self, features):\n",
    "\n",
    "        # running forward\n",
    "        output = self.build_graph(features)\n",
    "\n",
    "        # getting list value of the output\n",
    "        scores = output.npvalue()\n",
    "\n",
    "        # getting best tag\n",
    "        best_tag_id = np.argmax(scores)\n",
    "\n",
    "        # assigning the best tag\n",
    "        pred = self.vocab.tagid2tag_str(best_tag_id)\n",
    "\n",
    "        # refresh dynet memory (computation graph)\n",
    "        dynet.renew_cg()\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model.populate(filename)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from net_properties import *\n",
    "from utils import *\n",
    "from network import *\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    t0 = time.time()\n",
    "\n",
    "    we = 100\n",
    "    pe = 50\n",
    "    hidden = 200\n",
    "    minibatch = 1000\n",
    "    epochs = 5\n",
    "\n",
    "    net_properties = NetProperties(we, pe, hidden, minibatch)\n",
    "    vocab = Vocab(\"../data/train.data\")\n",
    "    pickle.dump((vocab, net_properties), open(\"../data/vocab_net.data\", 'wb'))\n",
    "    network = Network(vocab, net_properties)\n",
    "    network.train(\"../data/train.data\", epochs)\n",
    "    network.save(\"../networks/net.model\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    total = t1 - t0\n",
    "    print(\"runtime: \" + str(total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
